{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\be231\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# load basic libraries\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import neighbors, datasets\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import collections as ct\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import pos_tag,word_tokenize\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import naive_bayes ,svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, StratifiedKFold,train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ReadFile(inFile):\n",
    "    with open(inFile, \"r\") as f:\n",
    "        content = f.readlines()   \n",
    "    content = [x.strip() for x in content]\n",
    "    return (content)\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    #Utility function to clean tweet text by removing links, special characters using simple regex statements.\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "\n",
    "def Preprocessing(tweet,emotions,positive_Words,negative_Words,stop_Words):\n",
    "    for emotion in emotions:\n",
    "        tweet = tweet.replace(emotion[0], \" \"+emotion[1]+\" \",100)\n",
    "    #print (tweet)\n",
    "    buff = ''\n",
    "    for word in tweet.split():\n",
    "        if word not in stop_Words:\n",
    "            if word in positive_Words:\n",
    "                buff += \"positive \"\n",
    "                continue\n",
    "            elif word in negative_Words:\n",
    "                buff += \"negative \"\n",
    "                continue\n",
    "            else:\n",
    "                buff += nltk.stem.porter.PorterStemmer().stem(word)+\" \"\n",
    "    tweet = buff\n",
    "    tweet = clean_tweet(tweet)\n",
    "    \n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Type</th>\n",
       "      <th>Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.640000e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>I haven't money</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.630000e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>Theo Walcott is still shit\\u002c watch Rafa an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.620000e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>its not that I\\u2019m a GSP fan\\u002c i just h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.640000e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>Iranian general says Israel\\u2019s Iron Dome c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.630000e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Tehran\\u002c Mon Amour: Obama Tried to Establi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.640000e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>I sat through this whole movie just for Harry ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.640000e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>with J Davlar 11th. Main rivals are team Polan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.640000e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>Talking about ACT\\u2019s &amp;&amp; SAT\\u2019s\\u002c d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.120000e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Why is \\\"\"Happy Valentines Day\\\"\" trending? It...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.550000e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>They may have a SuperBowl in Dallas\\u002c but ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID      Type                                               Body\n",
       "0  2.640000e+17  positive                                    I haven't money\n",
       "1  2.630000e+17  negative  Theo Walcott is still shit\\u002c watch Rafa an...\n",
       "2  2.620000e+17  negative  its not that I\\u2019m a GSP fan\\u002c i just h...\n",
       "3  2.640000e+17  negative  Iranian general says Israel\\u2019s Iron Dome c...\n",
       "4  2.630000e+17   neutral  Tehran\\u002c Mon Amour: Obama Tried to Establi...\n",
       "5  2.640000e+17   neutral  I sat through this whole movie just for Harry ...\n",
       "6  2.640000e+17  positive  with J Davlar 11th. Main rivals are team Polan...\n",
       "7  2.640000e+17  negative  Talking about ACT\\u2019s && SAT\\u2019s\\u002c d...\n",
       "8  2.120000e+17   neutral  Why is \\\"\"Happy Valentines Day\\\"\" trending? It...\n",
       "9  2.550000e+17  negative  They may have a SuperBowl in Dallas\\u002c but ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "priPreprocessedPD = pd.read_csv(\"sumTraining.csv\",names=['ID','Type','Body'])\n",
    "priPreprocessedPD.head()\n",
    "#print (\"Positive instances:\" + str(len(np.where(priPreprocessedPD['Type'] == 'positive')[0])))\n",
    "#print (\"Negative instances:\" + str(len(np.where(priPreprocessedPD['Type'] == 'negative')[0])))\n",
    "#print (\"Neutral instances:\" + str(len(np.where(priPreprocessedPD['Type'] == 'neutral')[0])))\n",
    "priPreprocessedPD.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Type</th>\n",
       "      <th>Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.640000e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>I haven't money</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.630000e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>Theo Walcott is still shit\\u002c watch Rafa an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.620000e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>its not that I\\u2019m a GSP fan\\u002c i just h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.640000e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>Iranian general says Israel\\u2019s Iron Dome c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.630000e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Tehran\\u002c Mon Amour: Obama Tried to Establi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.640000e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>I sat through this whole movie just for Harry ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.640000e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>with J Davlar 11th. Main rivals are team Polan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.640000e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>Talking about ACT\\u2019s &amp;&amp; SAT\\u2019s\\u002c d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.120000e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Why is \\\"\"Happy Valentines Day\\\"\" trending? It...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.550000e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>They may have a SuperBowl in Dallas\\u002c but ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID      Type                                               Body\n",
       "0  2.640000e+17  positive                                    I haven't money\n",
       "1  2.630000e+17  negative  Theo Walcott is still shit\\u002c watch Rafa an...\n",
       "2  2.620000e+17  negative  its not that I\\u2019m a GSP fan\\u002c i just h...\n",
       "3  2.640000e+17  negative  Iranian general says Israel\\u2019s Iron Dome c...\n",
       "4  2.630000e+17   neutral  Tehran\\u002c Mon Amour: Obama Tried to Establi...\n",
       "5  2.640000e+17   neutral  I sat through this whole movie just for Harry ...\n",
       "6  2.640000e+17  positive  with J Davlar 11th. Main rivals are team Polan...\n",
       "7  2.640000e+17  negative  Talking about ACT\\u2019s && SAT\\u2019s\\u002c d...\n",
       "8  2.120000e+17   neutral  Why is \\\"\"Happy Valentines Day\\\"\" trending? It...\n",
       "9  2.550000e+17  negative  They may have a SuperBowl in Dallas\\u002c but ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "priPreprocessedPD.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10438    @eckoxsoldier For some reason Amazon app store...\n",
       "3251     \\u2019Now Behold the Lamb\\u002c the precious L...\n",
       "7935     #ESIAfrica Wescoal signs three year supply agr...\n",
       "9894     @sunnyvegas702 Nice! Disagreement! Of course, ...\n",
       "3344     Mee& my grandpa are going to the rita game tom...\n",
       "Name: Body, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_train,tweets_test,train_labels,test_labels = train_test_split(priPreprocessedPD[\"Body\"],priPreprocessedPD['Type'], test_size=0.2)\n",
    "tweets_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.40      0.02      0.03       471\n",
      "    neutral       0.49      0.49      0.49      1384\n",
      "   positive       0.46      0.62      0.53      1354\n",
      "\n",
      "avg / total       0.47      0.47      0.44      3209\n",
      "\n",
      "accuracy = 47.4602679963%\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "arr = []\n",
    "for train_idx, val_idx in kf.split(tweets_train, train_labels):\n",
    "    train_data = tweets_train.iloc[train_idx]\n",
    "    train_label = train_labels.iloc[train_idx]\n",
    "    val_data = tweets_train.iloc[val_idx]\n",
    "    val_label = train_labels.iloc[val_idx]\n",
    "    vectorizer = CountVectorizer()#TfidfVectorizer()#\n",
    "    X_train = vectorizer.fit_transform(train_data)\n",
    "    X_val = vectorizer.transform(val_data)\n",
    "    #clfr = naive_bayes.MultinomialNB()\n",
    "    #clfr = svm.SVC()\n",
    "    #clfr = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1)\n",
    "    clfr = neighbors.KNeighborsClassifier(15)\n",
    "    clfr.fit(X_train,train_label)\n",
    "    print ('#')\n",
    "\n",
    "tweets_test_vectorized = vectorizer.transform( tweets_test)\n",
    "predicted = clfr.predict(tweets_test_vectorized)\n",
    "acc = metrics.accuracy_score(test_labels,predicted)\n",
    "print (metrics.classification_report(test_labels,predicted))\n",
    "print('accuracy = '+str(acc*100)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.67      0.15      0.24       471\n",
      "    neutral       0.63      0.60      0.61      1384\n",
      "   positive       0.59      0.79      0.68      1354\n",
      "\n",
      "avg / total       0.62      0.61      0.59      3209\n",
      "\n",
      "accuracy =  61.140542225 %\n"
     ]
    }
   ],
   "source": [
    "clfrNaive = naive_bayes.MultinomialNB()\n",
    "clfrNaive.fit(X_train,train_label)\n",
    "predictedNaive = clfrNaive.predict(tweets_test_vectorized)\n",
    "acc = metrics.accuracy_score(test_labels,predictedNaive)\n",
    "print (metrics.classification_report(test_labels,predictedNaive))\n",
    "print('accuracy = ',acc*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.40      0.02      0.03       471\n",
      "    neutral       0.49      0.49      0.49      1384\n",
      "   positive       0.46      0.62      0.53      1354\n",
      "\n",
      "avg / total       0.47      0.47      0.44      3209\n",
      "\n",
      "accuracy =  47.4602679963 %\n"
     ]
    }
   ],
   "source": [
    "clr =  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
    "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
    "          verbose=0, warm_start=False)\n",
    "x = LogisticRegression()\n",
    "clfr.fit(X_train,train_label)\n",
    "predicted = clfr.predict(tweets_test_vectorized)\n",
    "acc = metrics.accuracy_score(test_labels,predicted)\n",
    "print (metrics.classification_report(test_labels,predicted))\n",
    "print('accuracy = ',acc*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "#Logestic regression\n",
    "pipe = Pipeline([('classifier', LogisticRegression())])\n",
    "search_space = [{'classifier': [LogisticRegression()],\n",
    "                 'classifier__C': [0.001,0.01,0.1,1,10,100],\n",
    "                'classifier__n_jobs':[-1],\n",
    "                'classifier__multi_class':['ovr']}]\n",
    "               \n",
    "\n",
    "clf_gss = GridSearchCV(pipe, search_space, cv=5, verbose=0)\n",
    "\n",
    "best_model = clf_gss.fit(X_train,train_label)\n",
    "\n",
    "# View best model\n",
    "print (best_model.best_estimator_.get_params()['classifier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('classifier', RandomForestClassifier())])\n",
    "search_space = [{'classifier': [RandomForestClassifier()],\n",
    "                 'classifier__n_estimators': [10, 100, 1000],\n",
    "                 'classifier__max_features': [1, 2, 3]}]\n",
    "               \n",
    "\n",
    "clf_gss = GridSearchCV(pipe, search_space, cv=5, verbose=0)\n",
    "\n",
    "best_model = clf_gss.fit(X_train,train_label)\n",
    "\n",
    "# View best model\n",
    "print (best_model.best_estimator_.get_params()['classifier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
